#!/user/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2025/12/11 15:18
@Author  : tianshiyang
@File    : streaming.py
"""
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from pydantic import BaseModel

from provider import chatGptLLM, qwenLLM

class UserRequest(BaseModel):
    messages: list[HumanMessage | AIMessage | ToolMessage]


@tool
def get_weather(city: str):
    """
    è·å–ä¼ å…¥åŸå¸‚çš„å¤©æ°”ä¿¡æ¯
    Args:
        city: åŸå¸‚åç§°

    Returns:
        è¿”å›è¯¥åŸå¸‚çš„å¤©æ°”æƒ…å†µ
    """
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œæ¸©åº¦ 15Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯å¥½",
        "ä¸Šæµ·": "å¤šäº‘ï¼Œæ¸©åº¦ 18Â°Cï¼Œæœ‰è½»å¾®é›¾éœ¾",
        "æ·±åœ³": "é˜´å¤©ï¼Œæ¸©åº¦ 22Â°Cï¼Œå¯èƒ½æœ‰å°é›¨",
        "æˆéƒ½": "å°é›¨ï¼Œæ¸©åº¦ 12Â°Cï¼Œæ¹¿åº¦è¾ƒé«˜"
    }
    return weather_data.get(city, f"æŠ±æ­‰ï¼Œæš‚æ—¶æ²¡æœ‰{city}çš„å¤©æ°”æ•°æ®")

@tool
def calculator(operation: str, a: float, b: float) -> str:
    """
    æ‰§è¡ŒåŸºæœ¬çš„æ•°å­¦è®¡ç®—

    å‚æ•°:
        operation: è¿ç®—ç±»å‹ï¼Œæ”¯æŒ "add"(åŠ ), "subtract"(å‡), "multiply"(ä¹˜), "divide"(é™¤)
        a: ç¬¬ä¸€ä¸ªæ•°å­—
        b: ç¬¬äºŒä¸ªæ•°å­—

    è¿”å›:
        è®¡ç®—ç»“æœå­—ç¬¦ä¸²
    """
    operations = {
        "add": lambda x, y: x + y,
        "subtract": lambda x, y: x - y,
        "multiply": lambda x, y: x * y,
        "divide": lambda x, y: x / y if y != 0 else "é”™è¯¯ï¼šé™¤æ•°ä¸èƒ½ä¸ºé›¶"
    }

    if operation not in operations:
        return f"ä¸æ”¯æŒçš„è¿ç®—ç±»å‹ï¼š{operation}ã€‚æ”¯æŒçš„ç±»å‹ï¼šadd, subtract, multiply, divide"

    try:
        result = operations[operation](a, b)
        return f"{a} {operation} {b} = {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯ï¼š{e}"

def example_1_stream_mode_values():
    """
    ç¤ºä¾‹1ï¼šä½¿ç”¨ stream_mode="values"

    è¿™æ˜¯é»˜è®¤æ¨¡å¼ï¼Œæ¯ä¸ªæ­¥éª¤åè¿”å›å®Œæ•´çš„çŠ¶æ€å­—å…¸ã€‚

    âš ï¸ é‡è¦ï¼šmessages åˆ—è¡¨ä¼šéšç€ Agent æ‰§è¡Œä¸æ–­å¢é•¿ï¼
    æ‰€ä»¥éœ€è¦ç”¨ messages[-1] æ¥è·å–æœ€æ–°æ·»åŠ çš„æ¶ˆæ¯ï¼
    """
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹ 1ï¼šstream_mode='values'ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰")
    print("=" * 70)

    agent = create_agent(
        qwenLLM,
        tools=[calculator]
    )

    print("\né—®é¢˜ï¼š25 ä¹˜ä»¥ 8 ç­‰äºå¤šå°‘ï¼Ÿ")
    print("\næµå¼è¾“å‡ºï¼ˆvalues æ¨¡å¼ï¼‰ï¼š")
    print("-" * 70)

    chunk_count = 0
    for chunk in agent.stream(
        UserRequest(messages=[HumanMessage("25 ä¹˜ä»¥ 8 ç­‰äºå¤šå°‘ï¼Ÿ")]),
        stream_mode="values"
    ):
        chunk_count += 1
        print("*"*60)
        print(f"chunk_count: {chunk_count}")
        print(f"ç±»å‹: {type(chunk)}")
        print(f"Chunk çš„é”®: {list(chunk.keys())}")

        if 'messages' in chunk:
            messages = chunk['messages']
            print(f"æ¶ˆæ¯æ€»æ•°: {len(messages)}")
            print(f"\nğŸ“‹ å½“å‰ messages åˆ—è¡¨ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼š")
            for i, msg in enumerate(chunk['messages'], 1):
                msg_type = msg.__class__.__name__
                print(f"  {i}. {msg_type}", end="")
                if hasattr(msg, 'content') and msg.content:
                    print(f" - {msg.content[:50]}...")
                elif hasattr(msg, 'tool_calls') and msg.tool_calls:
                    print(f" - è°ƒç”¨å·¥å…·: {msg.tool_calls[0]['name']}")
                else:
                    print()

        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
        last_message = chunk['messages'][-1]
        last_message_type = last_message.__class__.__name__
        print(f"æœ€æ–°æ¶ˆæ¯çš„ç±»å‹, {last_message_type}")
        if last_message_type == "AIMessage":
            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                print(f"AIå‡†å¤‡è°ƒç”¨å·¥å…·: {last_message.tool_calls[0]['name']}")
                print(f"  â†’ å·¥å…·å‚æ•°: {last_message.tool_calls[0]['args']}")
            elif hasattr(last_message, 'content') and last_message.content:
                print(f"  â†’ AI æœ€ç»ˆå›ç­”: {last_message.content[:100]}...")
        elif last_message_type == "ToolMessage":
            print(f"  â†’ å·¥å…·æ‰§è¡Œç»“æœ: {last_message.content}")

# ============================================================================
# ç¤ºä¾‹ 2ï¼šstream_mode="updates"
# ============================================================================
def example_2_stream_mode_updates():
    """

    è¿”å›çš„æ˜¯æ¯ä¸€ä¸ªå·¥å…·è¾“å‡ºçš„ç»“æœï¼Œå³AIMessage OR ToolMessage
    å…¶å®å°±æ˜¯æœ¬æ¬¡æ›´æ–°çš„ç»“æœ

    """
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹ 2ï¼šstream_mode='updates'")
    print("=" * 70)

    agent = create_agent(
        qwenLLM,
        tools=[calculator]
    )

    print("\né—®é¢˜ï¼š10 åŠ  20 ç­‰äºå¤šå°‘ï¼Ÿ")
    print("\næµå¼è¾“å‡ºï¼ˆupdates æ¨¡å¼ï¼‰ï¼š")
    print("-" * 70)

    chunks = agent.stream(
        UserRequest(messages=[HumanMessage("25 ä¹˜ä»¥ 8 ç­‰äºå¤šå°‘ï¼Ÿ")]),
        stream_mode="updates"
    )

    chunk_count = 0
    for chunk in chunks:
        chunk_count += 1
        print(f"\nã€Chunk {chunk_count}ã€‘")
        print(f"ç±»å‹: {type(chunk)}")
        print(f"Chunk çš„é”®ï¼ˆèŠ‚ç‚¹/å·¥å…·åï¼‰: {list(chunk.keys())}")

        for key, value  in chunk.items():
            cur_messages = value["messages"]
            print("*" * 60)
            print(f"æœ¬æ¬¡æ›´æ–°çš„æ¶ˆæ¯çš„æ•°é‡, {len(cur_messages)}")
            for i, msg in enumerate(cur_messages, 1):
                msg_type = msg.__class__.__name__
                print(f"æ¶ˆæ¯ç±»å‹ï¼š{msg_type}")
                if msg_type == "ToolMessage":
                    print(f"å·¥å…·è¿”å›çš„ç»“æœ: {msg.content[:50]}...")
                elif msg_type == "AIMessage":
                    if hasattr(msg, "tool_calls") and msg.tool_calls:
                        print(f"å·¥å…·è°ƒç”¨åç§°: {msg.tool_calls[0]['name']}")
                        print(f"å·¥å…·è°ƒç”¨å‚æ•°: {msg.tool_calls[0]['args']}")
                    elif hasattr(msg, "content") and msg.content:
                        print(f"å·¥å…·è¿”å›çš„ç»“æœ: {msg.content[:50]}...")

# ============================================================================
# ç¤ºä¾‹ 3ï¼šstream_mode="messages"
# ============================================================================

def example_3_stream_mode_messages():
    """
    ç¤ºä¾‹3ï¼šä½¿ç”¨ stream_mode="messages"

    é€ token è¿”å› LLM ç”Ÿæˆçš„æ¶ˆæ¯ï¼Œç±»ä¼¼æ‰“å­—æœºæ•ˆæœã€‚
    åŒæ—¶ä¹Ÿä¼šè¿”å›å·¥å…·è°ƒç”¨ç›¸å…³çš„æ¶ˆæ¯ï¼ˆAIMessageChunk with tool_calls å’Œ ToolMessageï¼‰

    è¿”å›ç±»å‹ï¼štupleï¼ˆå…ƒç»„ï¼Œ2ä¸ªå…ƒç´ ï¼‰

    å­—æ®µç»“æ„ï¼š
    (
        message_chunk,  # ç¬¬ä¸€ä¸ªå…ƒç´ ï¼šAIMessageChunk(1. AIçœŸå®çš„å›å¤-token by token  2.å†³å®šè°ƒç”¨ä»€ä¹ˆå·¥å…·çš„ç»“æ„åŒ–è¾“å‡º )ã€ToolMessageï¼ˆå·¥å…·è°ƒç”¨è¿”å›ç»“æœï¼‰ ç­‰æ¶ˆæ¯å¯¹è±¡
        metadata       # ç¬¬äºŒä¸ªå…ƒç´ ï¼šdictï¼ŒåŒ…å«å…ƒæ•°æ®å’Œ LangGraph æ‰§è¡Œä¿¡æ¯
    )

    ç‰¹ç‚¹ï¼š
    - è¿”å›çš„æ˜¯å…ƒç»„ï¼Œä¸æ˜¯å­—å…¸
    - ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æ¶ˆæ¯å¯¹è±¡ï¼ˆAIMessageChunkã€ToolMessage ç­‰ï¼‰
    - ç¬¬äºŒä¸ªå…ƒç´ æ˜¯å…ƒæ•°æ®ï¼ˆåŒ…å« LangGraph æ‰§è¡Œä¿¡æ¯å’Œæ¨¡å‹ä¿¡æ¯ï¼‰
    - ä¼šè¿”å› Agent æ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œæœ€ç»ˆç­”æ¡ˆ
    - é€‚åˆéœ€è¦å®æ—¶æ˜¾ç¤º AI å›ç­”çš„åœºæ™¯ï¼ˆå¦‚èŠå¤©ç•Œé¢ï¼‰

    """
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹ 3ï¼šstream_mode='messages'")
    print("=" * 70)

    agent = create_agent(
        qwenLLM,
        tools=[calculator]
    )

    print("\né—®é¢˜ï¼š1 åŠ  2 ç­‰äºå¤šå°‘ï¼Ÿ")
    print("\næµå¼è¾“å‡ºï¼ˆmessages æ¨¡å¼ï¼‰ï¼š")
    print("-" * 70)

    chunk_count = 0
    current_step = None
    current_node = None
    full_content = ""

    # Token ç»Ÿè®¡
    tool_call_tokens = None  # å·¥å…·è°ƒç”¨é˜¶æ®µçš„ token ç»Ÿè®¡
    final_answer_tokens = None  # æœ€ç»ˆç­”æ¡ˆé˜¶æ®µçš„ token ç»Ÿè®¡
    for chunk in agent.stream(
        UserRequest(messages=[HumanMessage("1+2ç­‰äºå¤šå°‘")]),
        stream_mode="messages"
    ):
        chunk_count += 1
        # messages æ¨¡å¼è¿”å›çš„æ˜¯å…ƒç»„ (message_chunk, metadata)
        if isinstance(chunk, tuple) and len(chunk) == 2:
            message_chunk, metadata = chunk

            # è·å–langgraphä¿¡æ¯
            step = metadata.get("langgraph_step", 'N/A')
            node = metadata.get('langgraph_node', 'N/A')

            # å¦‚æœæ­¥éª¤æˆ–èŠ‚ç‚¹å˜åŒ–ï¼Œæ˜¾ç¤ºæç¤º
            if step != current_step or node != current_node:
                if current_step is not None:
                    print()  # æ¢è¡Œ
                print(f"\nã€æ­¥éª¤ {step} - èŠ‚ç‚¹: {node}ã€‘")
                current_step = step
                current_node = node

            # æ ¹æ®æ¶ˆæ¯ç±»å‹å¤„ç†
            msg_type = message_chunk.__class__.__name__
            if msg_type == "AIMessageChunk":
                # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨
                if hasattr(message_chunk, "tool_calls") and message_chunk.tool_calls:
                    print(f"  â†’ AI å†³å®šè°ƒç”¨å·¥å…·: {message_chunk.tool_calls[0]['name']}")
                    print(f"  â†’ å·¥å…·å‚æ•°: {message_chunk.tool_calls[0]['args']}")
                elif hasattr(message_chunk, 'content') and message_chunk.content:
                    # å®æ—¶æ‰“å°å†…å®¹ï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰
                    print(message_chunk.content, end="", flush=True)
                    full_content += message_chunk.content

            elif msg_type == "ToolMessage":
                print(f"\n  â†’ å·¥å…·æ‰§è¡Œç»“æœ: {message_chunk.content}")
                print(f"  â†’ å·¥å…·åç§°: {message_chunk.name}")
                # âš ï¸ æ³¨æ„ï¼šToolMessage æ²¡æœ‰ token ç»Ÿè®¡ï¼ˆå·¥å…·æ‰§è¡Œä¸æ¶ˆè€— LLM tokenï¼‰

            # æ£€æŸ¥æ˜¯å¦æœ‰ usage_metadataï¼ˆtoken ç»Ÿè®¡ï¼‰ï¼Œæœ€åä¸€ä¸ªchunkå†…å®¹ä¸º'',å¹¶ä¸”æœ‰usage_metadataå­—æ®µ
            if hasattr(message_chunk, "usage_metadata") and message_chunk.usage_metadata:
                usage = message_chunk.usage_metadata
                input_tokens = usage.get('input_tokens', 0)
                output_tokens = usage.get('output_tokens', 0)
                total_tokens = usage.get('total_tokens', 0)

                # åˆ¤æ–­æ˜¯å·¥å…·è°ƒç”¨é˜¶æ®µè¿˜æ˜¯æœ€ç»ˆç­”æ¡ˆé˜¶æ®µ
                finish_reason = message_chunk.response_metadata.get('finish_reason', '') if hasattr(message_chunk,
                                                                                                    'response_metadata') else ''
                if finish_reason == 'tool_calls':
                    # å·¥å…·è°ƒç”¨é˜¶æ®µçš„ token ç»Ÿè®¡
                    tool_call_tokens = usage
                    print(
                        f"  â†’ [å·¥å…·è°ƒç”¨é˜¶æ®µç»“æŸ] Token ä½¿ç”¨: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}, æ€»è®¡={total_tokens}")
                else:
                    # æœ€ç»ˆç­”æ¡ˆé˜¶æ®µçš„ token ç»Ÿè®¡
                    final_answer_tokens = usage
                    print(
                        f"  â†’ [æœ€ç»ˆç­”æ¡ˆé˜¶æ®µ] Token ä½¿ç”¨: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}, æ€»è®¡={total_tokens}")

    print("\n\n" + "-" * 70)
    print(f"å®Œæ•´å›ç­”: {full_content}")
    print(f"æ€» chunk æ•°: {chunk_count}")

    # æ˜¾ç¤º Token ç»Ÿè®¡æ€»ç»“
    print("\n" + "-" * 70)
    print("ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡ï¼š")
    if tool_call_tokens:
        print(f"  å·¥å…·è°ƒç”¨é˜¶æ®µ: {tool_call_tokens.get('total_tokens', 0)} tokens")
    if final_answer_tokens:
        print(f"  æœ€ç»ˆç­”æ¡ˆé˜¶æ®µ: {final_answer_tokens.get('total_tokens', 0)} tokens")
        # âš ï¸ æ³¨æ„ï¼šæœ€ç»ˆç­”æ¡ˆé˜¶æ®µçš„ total_tokens å·²ç»åŒ…å«äº†å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨é˜¶æ®µï¼‰
        # æ‰€ä»¥è¿™æ˜¯æœ¬æ¬¡è¯·æ±‚çš„æ€» token æ¶ˆè€—
        print(f"  âš ï¸ æ€» Token æ¶ˆè€—: {final_answer_tokens.get('total_tokens', 0)} tokens")
        print(f"    ï¼ˆæœ€ç»ˆç­”æ¡ˆé˜¶æ®µçš„ total_tokens å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰")



if __name__ == '__main__':
    # example_1_stream_mode_values()
    # example_2_stream_mode_updates()
    example_3_stream_mode_messages()
    # TODO, Updateã€Valuesæ€ä¹ˆè·å–token